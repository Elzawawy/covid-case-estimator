{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting the Number of Daily Cases Using Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NTZTgd3irIeJ",
        "BvBR4phzrIej",
        "0whHQeak8TFZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LETUHTgj3SW",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Daily Cases Per Country\n",
        "In this notebook data of a number of countries is used to train a model per each country to be used to predict the daily cases on a specifc day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFQdD7qa0R67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from pandas import DataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WExuatl4mIUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "countries = {\"Germany\", \"France\", \"Italy\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYIbGPmUkOx4",
        "colab_type": "text"
      },
      "source": [
        "## Loading the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc8G9Vpr9Qhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_pickle(file_name):\n",
        "  with open(file_name, 'rb') as f:\n",
        "    return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x4VvmwvmcWK",
        "colab_type": "text"
      },
      "source": [
        "### Mounting Google Drive\n",
        "We mount google drive to access the data stored there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuUsNok0kWrp",
        "colab_type": "code",
        "outputId": "bb8ff3f1-bb54-4e73-c566-64d7d7d7e514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./gdrive')"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at ./gdrive; to attempt to forcibly remount, call drive.mount(\"./gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgrzYf_bmk0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_base_path = \"./gdrive/My Drive\"\n",
        "data_path = \"{}/COVID-19\".format(drive_base_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trsVS0h_lpc5",
        "colab_type": "text"
      },
      "source": [
        "### Loading the Weather Data of the Countries and Joining Them\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW6FLmZTmuj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weather_data_base_path = \"{}/weather-features\".format(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMFMIOOY6Zov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wind_speed_dict = dict(filter(lambda x: x[0] in countries, load_pickle(\"{}/windspeedKmph_dict.pickle\".format(weather_data_base_path)).items()))\n",
        "tempreture_dict = dict(filter(lambda x: x[0] in countries, load_pickle(\"{}/tempC_dict.pickle\".format(weather_data_base_path)).items()))\n",
        "humidity_dict = dict(filter(lambda x: x[0] in countries, load_pickle(\"{}/humidity_dict.pickle\".format(weather_data_base_path)).items()))\n",
        "sun_hour_dict = dict(filter(lambda x: x[0] in countries, load_pickle(\"{}/sunHour_dict.pickle\".format(weather_data_base_path)).items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NRz-ZjnSSFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "germany_df = DataFrame()\n",
        "germany_df['wind_speed'] = wind_speed_dict['Germany']\n",
        "germany_df['tempreture'] = tempreture_dict['Germany']\n",
        "germany_df['humidity'] = humidity_dict['Germany']\n",
        "germany_df['sun_hour'] = sun_hour_dict['Germany']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxm_4Sc9Swvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "italy_df = DataFrame()\n",
        "italy_df['wind_speed'] = wind_speed_dict['Italy']\n",
        "italy_df['tempreture'] = tempreture_dict['Italy']\n",
        "italy_df['humidity'] = humidity_dict['Italy']\n",
        "italy_df['sun_hour'] = sun_hour_dict['Italy']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc2uFe_kTA-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "france_df = DataFrame()\n",
        "france_df['wind_speed'] = wind_speed_dict['France']\n",
        "france_df['tempreture'] = tempreture_dict['France']\n",
        "france_df['humidity'] = humidity_dict['France']\n",
        "france_df['sun_hour'] = sun_hour_dict['France']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6BnFcvLUO5Z",
        "colab_type": "text"
      },
      "source": [
        "### Loading \"Our World in Data\" Dataset and Merging it With the Weather Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnBbtawJUOT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "our_world_data_base_path = \"{}/our-world-in-data\".format(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY3yWvjp19l-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_dataframes_filter_with_date(main_df, to_be_merged_df, column_names, start_date, end_date):\n",
        "  to_be_merged_df['date'] = pd.to_datetime(to_be_merged_df['date'])\n",
        "  to_be_merged_df = to_be_merged_df[(to_be_merged_df['date'] >= start_date) & (to_be_merged_df['date'] <= end_date)]\n",
        "  to_be_merged_df.index = [x for x in range(main_df.shape[0])]\n",
        "  main_df['date'] = to_be_merged_df['date']\n",
        "  for name in column_names:\n",
        "    main_df[name] = to_be_merged_df[name]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSYfvsBHY735",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_cases_dict = dict(filter(lambda x: x[0] in countries, load_pickle(\"{}/new_cases_dict.pickle\".format(our_world_data_base_path)).items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fw2GMxlWl43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_deaths_dict = dict(filter(lambda x: x[0] in countries, load_pickle(\"{}/new_deaths_dict.pickle\".format(our_world_data_base_path)).items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzM-D5fvC2Fa",
        "colab_type": "text"
      },
      "source": [
        "We have to restrict the days of the data to be the same as the days of weather data to be able to join them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9m0qRXIadtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_date = '2020-01-22'\n",
        "end_date = '2020-03-21'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEgtzoxFW7GX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "germany_new_cases_df = DataFrame(new_cases_dict['Germany'], columns=['date', 'new_cases'])\n",
        "merge_dataframes_filter_with_date(germany_df, germany_new_cases_df, ['new_cases'], start_date, end_date)\n",
        "germany_new_deaths_df = DataFrame(new_deaths_dict['Germany'], columns=['date', 'new_deaths'])\n",
        "merge_dataframes_filter_with_date(germany_df, germany_new_deaths_df, ['new_deaths'], start_date, end_date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv119E1n5-zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "italy_new_cases_df = DataFrame(new_cases_dict['Italy'], columns=['date', 'new_cases'])\n",
        "merge_dataframes_filter_with_date(italy_df, italy_new_cases_df, ['new_cases'], start_date, end_date)\n",
        "italy_new_deaths_df = DataFrame(new_deaths_dict['Italy'], columns=['date', 'new_deaths'])\n",
        "merge_dataframes_filter_with_date(italy_df, italy_new_deaths_df, ['new_deaths'], start_date, end_date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4JfUi2GZyl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "france_new_cases_df = DataFrame(new_cases_dict['France'], columns=['date', 'new_cases'])\n",
        "merge_dataframes_filter_with_date(france_df, france_new_cases_df, ['new_cases'], start_date, end_date)\n",
        "france_new_deaths_df = DataFrame(new_deaths_dict['France'], columns=['date', 'new_deaths'])\n",
        "merge_dataframes_filter_with_date(france_df, france_new_deaths_df, ['new_deaths'], start_date, end_date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21YA7YlEoXRW",
        "colab_type": "text"
      },
      "source": [
        "# Defining the Models\n",
        "In this section we are going to try 2 approaches.<br />\n",
        "The first one is to consider the datat as a time series and to use LSTM in the model and train it to predict the cases of a day based on a previous day. The time series forecasting problem can be trained to have differnet types of inputs and outputs. One approach is to input a day and output a day. A second approach is to have an input sequence of days and output a day. Another approach is to have a input sequence of days and output another sequence of days. We are going to use the first approach with the input day as one day before the output day. The number of days the input is before the output day is a hyper parameter that can be changed to get the best results.\n",
        "This [paper](https://www.researchgate.net/publication/341089678_Neural_Network_Model_for_Prediction_of_Covid-19_Confirmed_Cases_and_Fatalities) is used as a reference for the parameters used in the LSTM model.<br />\n",
        "The second approach we are going to use is a fully connected neural network with one hidden layer.\n",
        "We use root mean sqaure error as the error function to evaluate the models.<br />\n",
        "The models are tested using Germany, France and Italy's data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyngBWA6-m0S",
        "colab_type": "text"
      },
      "source": [
        "## LSTM\n",
        "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. Intuitively, the cell is responsible for keeping track of the dependencies between the elements in the input sequence. The input gate controls the extent to which a new value flows into the cell, the forget gate controls the extent to which a value remains in the cell and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit. The activation function of the LSTM gates is often the logistic sigmoid function.\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HmV0vc8qS7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import *\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzgNhBd3qOiq",
        "colab_type": "text"
      },
      "source": [
        "## Germany's Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLSPpMCGaOPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "germany_df.drop(columns='date', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QEiDe8Lmx0v",
        "colab_type": "text"
      },
      "source": [
        "### The LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOWowraV8Hi3",
        "colab_type": "text"
      },
      "source": [
        "#### Preparing the train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEuL9HE_DJ7o",
        "colab_type": "text"
      },
      "source": [
        "First thing to do is to add a new column wich is a shift by one day of the new cases model to used as the output for the LSTM model for the training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlImdj6JbpUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "germany_df['output_new_cases'] =germany_df['new_cases'].shift(-1)\n",
        "germany_df.drop(axis=0,index=[germany_df.shape[0]-1], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_oaW0HZDasg",
        "colab_type": "text"
      },
      "source": [
        "We split the data into a training, test and valdiation sets 70%, 10%, 20% splits respectively, after normalizing the data using MinMaxScaler in order to remove the dominance of large valued features. The sets have o be reashaped to [examples, timesteps, features] to be used as inputs to the LSTM model. In our case the timesteps used is 1 as only 1 day is considered as input not a sequence of days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYT8Vpedo4Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "germany_dataset = germany_df.values\n",
        "germany_dataset = germany_dataset.astype('float32')\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "germany_dataset = scaler.fit_transform(germany_dataset)\n",
        "train_size = int(len(germany_dataset) * 0.70)\n",
        "val_size = int(len(germany_dataset) * 0.10)\n",
        "test_size = int(len(germany_dataset) * 0.20)\n",
        "train, val ,test = germany_dataset[0:train_size,:], germany_dataset[train_size:val_size + train_size,:], germany_dataset[val_size + train_size:len(germany_dataset),:]\n",
        "train_X, train_Y = train[:,:-1], train[:,-1]\n",
        "test_X, test_Y = test[:, :-1], test[:, -1]\n",
        "val_X, val_Y = val[:, :-1], val[:, -1]\n",
        "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
        "val_X = np.reshape(val_X, (val_X.shape[0], 1, val_X.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFZzuppIcGve",
        "colab_type": "text"
      },
      "source": [
        "#### Training the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-J_N3G-o5Vk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "977315da-3047-4853-9860-92a44567e24e"
      },
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(64, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "lstm_model.add(Dropout(0.5))\n",
        "lstm_model.add(Dense(1))\n",
        "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history = lstm_model.fit(train_X, train_Y, epochs=20, batch_size=70, verbose=1, shuffle=False, \n",
        "                         validation_data=(val_X, val_Y), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "lstm_model.summary()"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 41 samples, validate on 5 samples\n",
            "Epoch 1/20\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 7.6273e-04\n",
            "Epoch 2/20\n",
            "41/41 [==============================] - 0s 147us/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 3/20\n",
            "41/41 [==============================] - 0s 125us/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 4/20\n",
            "41/41 [==============================] - 0s 91us/step - loss: 0.0019 - val_loss: 0.0022\n",
            "Epoch 5/20\n",
            "41/41 [==============================] - 0s 97us/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 6/20\n",
            "41/41 [==============================] - 0s 94us/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 7/20\n",
            "41/41 [==============================] - 0s 104us/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 8/20\n",
            "41/41 [==============================] - 0s 90us/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 9/20\n",
            "41/41 [==============================] - 0s 97us/step - loss: 9.8731e-04 - val_loss: 0.0016\n",
            "Epoch 10/20\n",
            "41/41 [==============================] - 0s 89us/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 11/20\n",
            "41/41 [==============================] - 0s 97us/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_9 (LSTM)                (None, 64)                18432     \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 18,497\n",
            "Trainable params: 18,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5G-w-hR6gFK",
        "colab_type": "text"
      },
      "source": [
        "#### Predicting Using the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcoDsGdIgw2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ceb16d0-5366-4abb-92ab-eff7ac9b457f"
      },
      "source": [
        "# Make a prediction\n",
        "yhat = lstm_model.predict(test_X)\n",
        "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "test_Y = test_Y.reshape((len(test_Y), 1))\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(test_Y, yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCpMe_1rm2f9",
        "colab_type": "text"
      },
      "source": [
        "### The Dense Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbh_uc9nLny",
        "colab_type": "text"
      },
      "source": [
        "#### Preparing the Train and Test Sets\n",
        "The data is split and normalized as done before the LSTM model but not reshaped as the input doesn't contain the timesteps dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWFVaY6HAoen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dense_model_germany_df = germany_df\n",
        "#Add a counter as a feature to indicate the day from the start of the pandemic\n",
        "dense_model_germany_df['days_from_pandemic_start'] = [x for x in range(dense_model_germany_df.shape[0])]\n",
        "g_new_cases_df = dense_model_germany_df['new_cases']\n",
        "dense_model_germany_df.drop(columns='new_cases', inplace=True)\n",
        "dense_model_germany_df['new_cases'] = g_new_cases_df\n",
        "germany_dense_model_dataset = dense_model_germany_df.values\n",
        "germany_dense_model_dataset = germany_dense_model_dataset.astype('float32')\n",
        "dense_model_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "germany_dense_model_dataset = dense_model_scaler.fit_transform(germany_dense_model_dataset)\n",
        "train_size = int(len(germany_dataset) * 0.70)\n",
        "val_size = int(len(germany_dataset) * 0.10)\n",
        "test_size = int(len(germany_dataset) * 0.20)\n",
        "train, val ,test = germany_dense_model_dataset[0:train_size,:], germany_dense_model_dataset[train_size:val_size + train_size,:], germany_dense_model_dataset[val_size + train_size:len(germany_dataset),:]\n",
        "train_X, train_Y = train[:,:-1], train[:,-1]\n",
        "test_X, test_Y = test[:, :-1], test[:, -1]\n",
        "val_X, val_Y = val[:, :-1], val[:, -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAv6bF168lAY",
        "colab_type": "text"
      },
      "source": [
        "#### Defining the Dense Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ7LW8UZ8D-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "3bb680b0-8b7c-40d4-a10f-c20faf76449a"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history = model.fit(train_X, train_Y, epochs=20, batch_size=70, verbose=1, shuffle=False, \n",
        "                    validation_data=(val_X, val_Y), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 41 samples, validate on 5 samples\n",
            "Epoch 1/20\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.0706 - val_loss: 0.0497\n",
            "Epoch 2/20\n",
            "41/41 [==============================] - 0s 124us/step - loss: 0.0712 - val_loss: 0.0421\n",
            "Epoch 3/20\n",
            "41/41 [==============================] - 0s 97us/step - loss: 0.0471 - val_loss: 0.0349\n",
            "Epoch 4/20\n",
            "41/41 [==============================] - 0s 104us/step - loss: 0.0694 - val_loss: 0.0278\n",
            "Epoch 5/20\n",
            "41/41 [==============================] - 0s 97us/step - loss: 0.0540 - val_loss: 0.0220\n",
            "Epoch 6/20\n",
            "41/41 [==============================] - 0s 101us/step - loss: 0.0663 - val_loss: 0.0168\n",
            "Epoch 7/20\n",
            "41/41 [==============================] - 0s 102us/step - loss: 0.0579 - val_loss: 0.0126\n",
            "Epoch 8/20\n",
            "41/41 [==============================] - 0s 56us/step - loss: 0.0401 - val_loss: 0.0095\n",
            "Epoch 9/20\n",
            "41/41 [==============================] - 0s 59us/step - loss: 0.0221 - val_loss: 0.0073\n",
            "Epoch 10/20\n",
            "41/41 [==============================] - 0s 108us/step - loss: 0.0332 - val_loss: 0.0059\n",
            "Epoch 11/20\n",
            "41/41 [==============================] - 0s 58us/step - loss: 0.0424 - val_loss: 0.0052\n",
            "Epoch 12/20\n",
            "41/41 [==============================] - 0s 61us/step - loss: 0.0367 - val_loss: 0.0051\n",
            "Epoch 13/20\n",
            "41/41 [==============================] - 0s 54us/step - loss: 0.0199 - val_loss: 0.0056\n",
            "Epoch 14/20\n",
            "41/41 [==============================] - 0s 69us/step - loss: 0.0332 - val_loss: 0.0063\n",
            "Epoch 15/20\n",
            "41/41 [==============================] - 0s 56us/step - loss: 0.0250 - val_loss: 0.0073\n",
            "Epoch 16/20\n",
            "41/41 [==============================] - 0s 58us/step - loss: 0.0224 - val_loss: 0.0084\n",
            "Epoch 17/20\n",
            "41/41 [==============================] - 0s 57us/step - loss: 0.0219 - val_loss: 0.0096\n",
            "Epoch 18/20\n",
            "41/41 [==============================] - 0s 60us/step - loss: 0.0182 - val_loss: 0.0110\n",
            "Epoch 19/20\n",
            "41/41 [==============================] - 0s 75us/step - loss: 0.0209 - val_loss: 0.0122\n",
            "Epoch 20/20\n",
            "41/41 [==============================] - 0s 60us/step - loss: 0.0197 - val_loss: 0.0133\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_32 (Dense)             (None, 64)                512       \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 577\n",
            "Trainable params: 577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irUTusg5_67l",
        "colab_type": "text"
      },
      "source": [
        "#### Predicting Using the Dense Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTWzQUJd-3xw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dda8f427-7544-4211-e0c4-c3d1bb9758dc"
      },
      "source": [
        "# make a prediction\n",
        "yhat = model.predict(test_X)\n",
        "test_Y = test_Y.reshape((len(test_Y), 1))\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(test_Y, yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhhrhBrQF5nZ",
        "colab_type": "text"
      },
      "source": [
        "## The same steps are repeated for the France and the Italy's datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NTZTgd3irIeJ"
      },
      "source": [
        "## France's Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9_It1YUPrIeT",
        "colab": {}
      },
      "source": [
        "france_df.drop(columns='date', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BvBR4phzrIej"
      },
      "source": [
        "### The LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iJHezWYhrIem"
      },
      "source": [
        "#### Preparing the train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y6MSHrgLrIeo",
        "colab": {}
      },
      "source": [
        "france_df['output_new_cases'] =france_df['new_cases'].shift(-1)\n",
        "france_df.drop(axis=0,index=[france_df.shape[0]-1], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ie32uzAwrIfD",
        "colab": {}
      },
      "source": [
        "france_dataset = france_df.values\n",
        "france_dataset = france_dataset.astype('float32')\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "france_dataset = scaler.fit_transform(france_dataset)\n",
        "train_size = int(len(france_dataset) * 0.70)\n",
        "val_size = int(len(france_dataset) * 0.10)\n",
        "test_size = int(len(france_dataset) * 0.20)\n",
        "train, val ,test = france_dataset[0:train_size,:], france_dataset[train_size:val_size + train_size,:], france_dataset[val_size + train_size:len(france_dataset),:]\n",
        "train_X, train_Y = train[:,:-1], train[:,-1]\n",
        "test_X, test_Y = test[:, :-1], test[:, -1]\n",
        "val_X, val_Y = val[:, :-1], val[:, -1]\n",
        "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
        "val_X = np.reshape(val_X, (val_X.shape[0], 1, val_X.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JVOn5bz6rIfK"
      },
      "source": [
        "#### Defining the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YLWahFOvrIfN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "outputId": "5a5a53f4-e6e4-4986-838c-cbb0a2c5638a"
      },
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(64, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "lstm_model.add(Dropout(0.5))\n",
        "lstm_model.add(Dense(1))\n",
        "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history = lstm_model.fit(train_X, train_Y, epochs=20, batch_size=70, verbose=1, shuffle=False, \n",
        "                         validation_data=(val_X, val_Y), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "lstm_model.summary()"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 41 samples, validate on 5 samples\n",
            "Epoch 1/20\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0048\n",
            "Epoch 2/20\n",
            "41/41 [==============================] - 0s 147us/step - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 3/20\n",
            "41/41 [==============================] - 0s 123us/step - loss: 8.8675e-04 - val_loss: 0.0038\n",
            "Epoch 4/20\n",
            "41/41 [==============================] - 0s 122us/step - loss: 6.7794e-04 - val_loss: 0.0034\n",
            "Epoch 5/20\n",
            "41/41 [==============================] - 0s 117us/step - loss: 9.4882e-04 - val_loss: 0.0033\n",
            "Epoch 6/20\n",
            "41/41 [==============================] - 0s 120us/step - loss: 6.8105e-04 - val_loss: 0.0032\n",
            "Epoch 7/20\n",
            "41/41 [==============================] - 0s 119us/step - loss: 8.0140e-04 - val_loss: 0.0032\n",
            "Epoch 8/20\n",
            "41/41 [==============================] - 0s 125us/step - loss: 6.9709e-04 - val_loss: 0.0033\n",
            "Epoch 9/20\n",
            "41/41 [==============================] - 0s 119us/step - loss: 7.9482e-04 - val_loss: 0.0034\n",
            "Epoch 10/20\n",
            "41/41 [==============================] - 0s 112us/step - loss: 6.1630e-04 - val_loss: 0.0036\n",
            "Epoch 11/20\n",
            "41/41 [==============================] - 0s 121us/step - loss: 7.0715e-04 - val_loss: 0.0038\n",
            "Epoch 12/20\n",
            "41/41 [==============================] - 0s 115us/step - loss: 5.5393e-04 - val_loss: 0.0041\n",
            "Epoch 13/20\n",
            "41/41 [==============================] - 0s 114us/step - loss: 4.4567e-04 - val_loss: 0.0043\n",
            "Epoch 14/20\n",
            "41/41 [==============================] - 0s 115us/step - loss: 7.5241e-04 - val_loss: 0.0044\n",
            "Epoch 15/20\n",
            "41/41 [==============================] - 0s 114us/step - loss: 5.2984e-04 - val_loss: 0.0045\n",
            "Epoch 16/20\n",
            "41/41 [==============================] - 0s 106us/step - loss: 5.0908e-04 - val_loss: 0.0045\n",
            "Epoch 17/20\n",
            "41/41 [==============================] - 0s 93us/step - loss: 5.5986e-04 - val_loss: 0.0043\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_12 (LSTM)               (None, 64)                18176     \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 18,241\n",
            "Trainable params: 18,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_2j3C0mrIfa"
      },
      "source": [
        "#### Predicting Using the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d701f51a-4cbc-415d-f9e1-69fbb1d69ecc",
        "id": "NMRltK55rIfk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make a prediction\n",
        "yhat = lstm_model.predict(test_X)\n",
        "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "test_Y = test_Y.reshape((len(test_Y), 1))\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(test_Y, yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2M1xQgtkrIft"
      },
      "source": [
        "### The Dense Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "57lbP_eGrIfv"
      },
      "source": [
        "#### Preparing the Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "au4dSbYZrIfw",
        "colab": {}
      },
      "source": [
        "dense_model_france_df = france_df\n",
        "#Add a counter as a feature to indicate the day from the start of the pandemic\n",
        "dense_model_france_df['days_from_pandemic_start'] = [x for x in range(dense_model_france_df.shape[0])]\n",
        "g_new_cases_df = dense_model_france_df['new_cases']\n",
        "dense_model_france_df.drop(columns='new_cases', inplace=True)\n",
        "dense_model_france_df['new_cases'] = g_new_cases_df\n",
        "france_dense_model_dataset = dense_model_france_df.values\n",
        "france_dense_model_dataset = france_dense_model_dataset.astype('float32')\n",
        "dense_model_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "france_dense_model_dataset = dense_model_scaler.fit_transform(france_dense_model_dataset)\n",
        "train_size = int(len(france_dataset) * 0.70)\n",
        "val_size = int(len(france_dataset) * 0.10)\n",
        "test_size = int(len(france_dataset) * 0.20)\n",
        "train, val ,test = france_dense_model_dataset[0:train_size,:], france_dense_model_dataset[train_size:val_size + train_size,:], france_dense_model_dataset[val_size + train_size:len(france_dataset),:]\n",
        "train_X, train_Y = train[:,:-1], train[:,-1]\n",
        "test_X, test_Y = test[:, :-1], test[:, -1]\n",
        "val_X, val_Y = val[:, :-1], val[:, -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6tjTp6CRrIf3"
      },
      "source": [
        "#### Defining the Dense Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bc98550f-f70b-436d-a9d9-8620e6da9e37",
        "id": "0hRNLdyIrIf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history = model.fit(train_X, train_Y, epochs=20, batch_size=70, verbose=1, shuffle=False, \n",
        "                    validation_data=(val_X, val_Y), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 41 samples, validate on 5 samples\n",
            "Epoch 1/20\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.0481 - val_loss: 0.0164\n",
            "Epoch 2/20\n",
            "41/41 [==============================] - 0s 86us/step - loss: 0.0414 - val_loss: 0.0128\n",
            "Epoch 3/20\n",
            "41/41 [==============================] - 0s 65us/step - loss: 0.0371 - val_loss: 0.0102\n",
            "Epoch 4/20\n",
            "41/41 [==============================] - 0s 70us/step - loss: 0.0397 - val_loss: 0.0083\n",
            "Epoch 5/20\n",
            "41/41 [==============================] - 0s 53us/step - loss: 0.0212 - val_loss: 0.0070\n",
            "Epoch 6/20\n",
            "41/41 [==============================] - 0s 59us/step - loss: 0.0224 - val_loss: 0.0060\n",
            "Epoch 7/20\n",
            "41/41 [==============================] - 0s 60us/step - loss: 0.0227 - val_loss: 0.0055\n",
            "Epoch 8/20\n",
            "41/41 [==============================] - 0s 57us/step - loss: 0.0214 - val_loss: 0.0053\n",
            "Epoch 9/20\n",
            "41/41 [==============================] - 0s 60us/step - loss: 0.0262 - val_loss: 0.0052\n",
            "Epoch 10/20\n",
            "41/41 [==============================] - 0s 56us/step - loss: 0.0264 - val_loss: 0.0053\n",
            "Epoch 11/20\n",
            "41/41 [==============================] - 0s 60us/step - loss: 0.0152 - val_loss: 0.0055\n",
            "Epoch 12/20\n",
            "41/41 [==============================] - 0s 63us/step - loss: 0.0178 - val_loss: 0.0057\n",
            "Epoch 13/20\n",
            "41/41 [==============================] - 0s 73us/step - loss: 0.0191 - val_loss: 0.0059\n",
            "Epoch 14/20\n",
            "41/41 [==============================] - 0s 65us/step - loss: 0.0222 - val_loss: 0.0059\n",
            "Epoch 15/20\n",
            "41/41 [==============================] - 0s 72us/step - loss: 0.0264 - val_loss: 0.0059\n",
            "Epoch 16/20\n",
            "41/41 [==============================] - 0s 69us/step - loss: 0.0199 - val_loss: 0.0057\n",
            "Epoch 17/20\n",
            "41/41 [==============================] - 0s 68us/step - loss: 0.0160 - val_loss: 0.0054\n",
            "Epoch 18/20\n",
            "41/41 [==============================] - 0s 72us/step - loss: 0.0245 - val_loss: 0.0051\n",
            "Epoch 19/20\n",
            "41/41 [==============================] - 0s 76us/step - loss: 0.0208 - val_loss: 0.0047\n",
            "Epoch 20/20\n",
            "41/41 [==============================] - 0s 81us/step - loss: 0.0115 - val_loss: 0.0043\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_39 (Dense)             (None, 64)                512       \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 577\n",
            "Trainable params: 577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mHrluPRyrIgB"
      },
      "source": [
        "#### Predicting Using the Dense Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d866cd2b-7290-4e58-b704-e0338d2f71b1",
        "id": "IMn6ae1vrIgB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make a prediction\n",
        "yhat = model.predict(test_X)\n",
        "test_Y = test_Y.reshape((len(test_Y), 1))\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(test_Y, yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YZMSKHgzrIgH",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0whHQeak8TFZ"
      },
      "source": [
        "## Italy's Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FQmkcLnn8TFh",
        "colab": {}
      },
      "source": [
        "italy_df.drop(columns='date', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-RvtfVLZ8TF3"
      },
      "source": [
        "### The LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f8pTWHPc8TF6"
      },
      "source": [
        "#### Preparing the train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Npaykk5K8TF8",
        "colab": {}
      },
      "source": [
        "italy_df['output_new_cases'] =italy_df['new_cases'].shift(-1)\n",
        "italy_df.drop(axis=0,index=[italy_df.shape[0]-1], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NdWUaX_e8TGJ",
        "colab": {}
      },
      "source": [
        "italy_dataset = france_df.values\n",
        "italy_dataset = italy_dataset.astype('float32')\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "italy_dataset = scaler.fit_transform(italy_dataset)\n",
        "train_size = int(len(italy_dataset) * 0.70)\n",
        "val_size = int(len(italy_dataset) * 0.10)\n",
        "test_size = int(len(italy_dataset) * 0.20)\n",
        "train, val ,test = italy_dataset[0:train_size,:], italy_dataset[train_size:val_size + train_size,:], italy_dataset[val_size + train_size:len(italy_dataset),:]\n",
        "train_X, train_Y = train[:,:-1], train[:,-1]\n",
        "test_X, test_Y = test[:, :-1], test[:, -1]\n",
        "val_X, val_Y = val[:, :-1], val[:, -1]\n",
        "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
        "val_X = np.reshape(val_X, (val_X.shape[0], 1, val_X.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VYGpehpT8TGU"
      },
      "source": [
        "#### Defining the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "662eaa6e-08e2-4650-9152-c2b0d194727d",
        "id": "1L7tyaE_8TGW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(64, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "lstm_model.add(Dropout(0.5))\n",
        "lstm_model.add(Dense(1))\n",
        "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history = lstm_model.fit(train_X, train_Y, epochs=20, batch_size=70, verbose=1, shuffle=False, \n",
        "                         validation_data=(val_X, val_Y), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "lstm_model.summary()"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 41 samples, validate on 5 samples\n",
            "Epoch 1/20\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 2/20\n",
            "41/41 [==============================] - 0s 149us/step - loss: 0.0024 - val_loss: 0.0032\n",
            "Epoch 3/20\n",
            "41/41 [==============================] - 0s 266us/step - loss: 0.0024 - val_loss: 0.0033\n",
            "Epoch 4/20\n",
            "41/41 [==============================] - 0s 145us/step - loss: 0.0011 - val_loss: 0.0033\n",
            "Epoch 5/20\n",
            "41/41 [==============================] - 0s 139us/step - loss: 0.0012 - val_loss: 0.0032\n",
            "Epoch 6/20\n",
            "41/41 [==============================] - 0s 109us/step - loss: 0.0011 - val_loss: 0.0031\n",
            "Epoch 7/20\n",
            "41/41 [==============================] - 0s 105us/step - loss: 0.0012 - val_loss: 0.0032\n",
            "Epoch 8/20\n",
            "41/41 [==============================] - 0s 110us/step - loss: 0.0012 - val_loss: 0.0031\n",
            "Epoch 9/20\n",
            "41/41 [==============================] - 0s 121us/step - loss: 9.4214e-04 - val_loss: 0.0030\n",
            "Epoch 10/20\n",
            "41/41 [==============================] - 0s 117us/step - loss: 0.0010 - val_loss: 0.0030\n",
            "Epoch 11/20\n",
            "41/41 [==============================] - 0s 116us/step - loss: 0.0011 - val_loss: 0.0028\n",
            "Epoch 12/20\n",
            "41/41 [==============================] - 0s 108us/step - loss: 8.6113e-04 - val_loss: 0.0027\n",
            "Epoch 13/20\n",
            "41/41 [==============================] - 0s 110us/step - loss: 8.3132e-04 - val_loss: 0.0026\n",
            "Epoch 14/20\n",
            "41/41 [==============================] - 0s 109us/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 15/20\n",
            "41/41 [==============================] - 0s 88us/step - loss: 7.8429e-04 - val_loss: 0.0028\n",
            "Epoch 16/20\n",
            "41/41 [==============================] - 0s 82us/step - loss: 6.3349e-04 - val_loss: 0.0030\n",
            "Epoch 17/20\n",
            "41/41 [==============================] - 0s 112us/step - loss: 8.1473e-04 - val_loss: 0.0031\n",
            "Epoch 18/20\n",
            "41/41 [==============================] - 0s 98us/step - loss: 0.0010 - val_loss: 0.0032\n",
            "Epoch 19/20\n",
            "41/41 [==============================] - 0s 97us/step - loss: 7.0902e-04 - val_loss: 0.0032\n",
            "Epoch 20/20\n",
            "41/41 [==============================] - 0s 124us/step - loss: 6.8554e-04 - val_loss: 0.0032\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 64)                18432     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 18,497\n",
            "Trainable params: 18,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uqYR0fkc8TGf"
      },
      "source": [
        "#### Predicting Using the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f76a6355-5b2f-4014-be02-3d78c2b79629",
        "id": "sFaI5pr88TGh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make a prediction\n",
        "yhat = lstm_model.predict(test_X)\n",
        "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "test_Y = test_Y.reshape((len(test_Y), 1))\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(test_Y, yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WBBWgpIu8TGq"
      },
      "source": [
        "### The Dense Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bJhM3qDO8TGr"
      },
      "source": [
        "#### Preparing the Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3DhGwwqZ8TGu",
        "colab": {}
      },
      "source": [
        "dense_model_italy_df = italy_df\n",
        "#Add a counter as a feature to indicate the day from the start of the pandemic\n",
        "dense_model_italy_df['days_from_pandemic_start'] = [x for x in range(dense_model_italy_df.shape[0])]\n",
        "g_new_cases_df = dense_model_italy_df['new_cases']\n",
        "dense_model_italy_df.drop(columns='new_cases', inplace=True)\n",
        "dense_model_italy_df['new_cases'] = g_new_cases_df\n",
        "italy_dense_model_dataset = dense_model_italy_df.values\n",
        "italy_dense_model_dataset = italy_dense_model_dataset.astype('float32')\n",
        "dense_model_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "italy_dense_model_dataset = dense_model_scaler.fit_transform(italy_dense_model_dataset)\n",
        "train_size = int(len(italy_dataset) * 0.70)\n",
        "val_size = int(len(italy_dataset) * 0.10)\n",
        "test_size = int(len(italy_dataset) * 0.20)\n",
        "train, val ,test = italy_dense_model_dataset[0:train_size,:], italy_dense_model_dataset[train_size:val_size + train_size,:], italy_dense_model_dataset[val_size + train_size:len(italy_dataset),:]\n",
        "train_X, train_Y = train[:,:-1], train[:,-1]\n",
        "test_X, test_Y = test[:, :-1], test[:, -1]\n",
        "val_X, val_Y = val[:, :-1], val[:, -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZgZPpDWo8TG5"
      },
      "source": [
        "#### Defining the Dense Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "23906714-7a38-46fc-9fde-46d4d07709d0",
        "id": "kl83p4oY8TG7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history = model.fit(train_X, train_Y, epochs=20, batch_size=70, verbose=1, shuffle=False, \n",
        "                    validation_data=(val_X, val_Y), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 41 samples, validate on 5 samples\n",
            "Epoch 1/20\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.1060\n",
            "Epoch 2/20\n",
            "41/41 [==============================] - 0s 69us/step - loss: 0.0552 - val_loss: 0.0952\n",
            "Epoch 3/20\n",
            "41/41 [==============================] - 0s 99us/step - loss: 0.0517 - val_loss: 0.0851\n",
            "Epoch 4/20\n",
            "41/41 [==============================] - 0s 80us/step - loss: 0.0436 - val_loss: 0.0764\n",
            "Epoch 5/20\n",
            "41/41 [==============================] - 0s 65us/step - loss: 0.0197 - val_loss: 0.0692\n",
            "Epoch 6/20\n",
            "41/41 [==============================] - 0s 66us/step - loss: 0.0359 - val_loss: 0.0623\n",
            "Epoch 7/20\n",
            "41/41 [==============================] - 0s 72us/step - loss: 0.0334 - val_loss: 0.0556\n",
            "Epoch 8/20\n",
            "41/41 [==============================] - 0s 64us/step - loss: 0.0275 - val_loss: 0.0496\n",
            "Epoch 9/20\n",
            "41/41 [==============================] - 0s 60us/step - loss: 0.0300 - val_loss: 0.0442\n",
            "Epoch 10/20\n",
            "41/41 [==============================] - 0s 64us/step - loss: 0.0321 - val_loss: 0.0394\n",
            "Epoch 11/20\n",
            "41/41 [==============================] - 0s 71us/step - loss: 0.0253 - val_loss: 0.0355\n",
            "Epoch 12/20\n",
            "41/41 [==============================] - 0s 66us/step - loss: 0.0258 - val_loss: 0.0323\n",
            "Epoch 13/20\n",
            "41/41 [==============================] - 0s 68us/step - loss: 0.0230 - val_loss: 0.0295\n",
            "Epoch 14/20\n",
            "41/41 [==============================] - 0s 69us/step - loss: 0.0269 - val_loss: 0.0272\n",
            "Epoch 15/20\n",
            "41/41 [==============================] - 0s 70us/step - loss: 0.0354 - val_loss: 0.0253\n",
            "Epoch 16/20\n",
            "41/41 [==============================] - 0s 65us/step - loss: 0.0158 - val_loss: 0.0237\n",
            "Epoch 17/20\n",
            "41/41 [==============================] - 0s 67us/step - loss: 0.0188 - val_loss: 0.0224\n",
            "Epoch 18/20\n",
            "41/41 [==============================] - 0s 68us/step - loss: 0.0283 - val_loss: 0.0216\n",
            "Epoch 19/20\n",
            "41/41 [==============================] - 0s 68us/step - loss: 0.0199 - val_loss: 0.0210\n",
            "Epoch 20/20\n",
            "41/41 [==============================] - 0s 68us/step - loss: 0.0146 - val_loss: 0.0206\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_46 (Dense)             (None, 64)                512       \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 577\n",
            "Trainable params: 577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LXisM4iN8THF"
      },
      "source": [
        "#### Predicting Using the Dense Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5f90d080-2873-49d4-bc3c-66ab4d9d7b5e",
        "id": "SjPozzk78THF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make a prediction\n",
        "yhat = model.predict(test_X)\n",
        "test_Y = test_Y.reshape((len(test_Y), 1))\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(test_Y, yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M15A81r9mUo",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion\n",
        "The results are close when using the LSTM model or the dense model with the dense model having slightly better results. The amount of the data was a limitation with only 60 days used as the weather data is the resitricting factor. Results would have been better and a better comparison would have been made if more data samples are used in training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7E6N1FTBkOt",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "*  https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "*   https://towardsdatascience.com/time-series-analysis-visualization-forecasting-with-lstm-77a905180eba\n",
        "*  https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
        "*  https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
        "*  https://towardsdatascience.com/lstm-for-time-series-prediction-de8aeb26f2ca\n",
        "*  https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
        "*  https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
        "\n",
        "\n"
      ]
    }
  ]
}